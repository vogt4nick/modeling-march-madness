{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python\n",
    "\n",
    "Note: I've made a Scraper class object for this tutorial. You can find it in the Scraper.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write raw html\n",
    "\n",
    "If you're writing and debugging code, store the data locally. There are two good reasons:\n",
    "1. Sending the same request to a server dozens or possibly hundreds of times puts unnecessary burden on their systems.\n",
    "2. It's faster to pull data from a local file.\n",
    "\n",
    "Always write encoded bytes. If you don't yet know why, [check out Ned Batchelder's PyCon talk](https://www.youtube.com/watch?v=sgHbC6udIqc). Watch every second of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "\n",
    "url = \"http://www.sports-reference.com/cbb/boxscores/\"\n",
    "# \"/../\" denotes the directory directly above the current working directory.\n",
    "path = './../html/today\\'s box scores.txt'\n",
    "\n",
    "# Make ID line\n",
    "ID = hex(random.randrange(16**30))\n",
    "ID_line = '<!-- ID: {} -->\\n'.format(ID).encode('utf-8')\n",
    "\n",
    "# Make request\n",
    "r = requests.get(url)\n",
    "\n",
    "# We're joining a byte string. We need to denote this with b\"\"\n",
    "html = b''.join((ID_line, r.content))\n",
    "\n",
    "with open(path, 'wb') as ofile:\n",
    "    ofile.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tip 1\n",
    "\n",
    "`r.content` returns encoded bytes (print `r.content` and notice the \"b\" before \"<!doctype html>...\")  \n",
    "`r.text` returns decoded utf-8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tip 2\n",
    "You can control more information sent by the request by creating a `requests` Session object. You'll want to change the headers if you're scraping more sophisticated webpages. Copy the headers used through your favorite VPN and use them with your session object! But use caution...\n",
    "\n",
    "```\n",
    "session = requests.Session()\n",
    "session.headers = {  # These are the default headers, for example\n",
    "    'Accept-Encoding': 'gzip, deflate', \n",
    "    'User-Agent': 'python-requests/2.10.0', \n",
    "    'Connection': 'keep-alive', \n",
    "    'Accept': '*/*'\n",
    "}\n",
    "r = session.get(url)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make the crawler _crawl_\n",
    "\n",
    "Note: We will cover `BeautifulSoup` more deeply in the data mining tutorial.\n",
    "\n",
    "We often want to follow internal links on webpages to get to more interesting data. This is _crawling_. To do this, we'll want to use `BeautifulSoup` to help us navigate HTML documents.\n",
    "\n",
    "If you get confused along the way, I encourage you to take [Codecademy's HTML Basics course](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiBv5OsnPnRAhVs9IMKHUYgAVwQFgglMAI&url=https%3A%2F%2Fwww.codecademy.com%2Fcourses%2Fweb-beginner-en-HZA3b&usg=AFQjCNHx-r5eaJMv2t-K7FNN3V_4bz7f9A&sig2=CVR05UOaXFlygapyXPeEaw). It's short and explains things better than I can in this workshop.\n",
    "\n",
    "Here's the plan of attack:\n",
    "\n",
    "1. Identify links in a webpage.\n",
    "2. Follow those links.\n",
    "\n",
    "### Quick Intro to BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<title>Nick’s Workshop</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>\n",
       "<a href=\"/workshop\">\n",
       "            Nick’s Workshop\n",
       "            </a>\n",
       "</h1>\n",
       "<h1>Welcome!</h1>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Nick’s Workshop</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>\n",
    "            <a href=\"/workshop\">\n",
    "            Nick’s Workshop\n",
    "            </a>\n",
    "        </h1>\n",
    "        <h1>Welcome!</h1>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can navigate the tree using html tags in one of two ways:\n",
    "\n",
    "1. BeautifulSoup attributes\n",
    "2. BeautifulSoup `find` and `find_all` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using attributes:\n",
      " <a href=\"/workshop\">\n",
      "            Nick’s Workshop\n",
      "            </a>\n",
      "Using the find method:\n",
      " <a href=\"/workshop\">\n",
      "            Nick’s Workshop\n",
      "            </a>\n"
     ]
    }
   ],
   "source": [
    "print(\"Using attributes:\\n\",      soup.h1.a)\n",
    "print(\"Using the find method:\\n\", soup.find('h1').find('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this will always find the first instance which matches the specifications. To find all matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the find_all method: [<h1>\n",
      "<a href=\"/workshop\">\n",
      "            Nick’s Workshop\n",
      "            </a>\n",
      "</h1>, <h1>Welcome!</h1>]\n"
     ]
    }
   ],
   "source": [
    "print(\"Using the find_all method:\", soup.find_all('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the internal link. To separate it from its attribute tag using the `get` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workshop'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h1.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tip 3\n",
    "\n",
    "The `find_all` method actually returns a `ResultSet` object which behaves similarly to a list. This means we can't repeat `find_all` functions like we do with `find`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResultSet' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    soup.find_all('h1').find_all('a')\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tip 4 \n",
    "In practice, we must further specify tags by their class, id, etc. The SelectorGadget makes easy work of this. For the sake of time, we'll save explanation of how to use it for the data mining tutorial.\n",
    "\n",
    "**Note**: The SelectorGadget is a chrome add-on. You can [check out the webpage and short tutorial](http://selectorgadget.com/) if you're really curious how to use it _now_. It's pretty intuitive if you're familiar with HTML classes and ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following Internal Links\n",
    "\n",
    "We now know how to find internal links on a page. There are two things we want to do next:\n",
    "\n",
    "1. Scrape the box scores for more game data;\n",
    "2. Find scores from different dates. \n",
    "\n",
    "We'll switch gears now to scraping [NCAA basketball box scores on sports-reference](http://www.sports-reference.com/cbb/boxscores/index.cgi?month=02&day=03&year=2017) to demonstrate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web pages contain absolute paths and relative paths. An absolute path contains the entire address to a link. A relative path contains the address relative to another base address. \n",
    "\n",
    "Internal links almost always contain relative paths. For example, the link to college basketball stats on [sports-reference](http://sports-reference.com) is \"/cbb\". The web pages knows this is an internal link and will send the user to http://sports-reference.com/cbb. \n",
    "\n",
    "To get the box score data, we must make a similar correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cbb/boxscores/2017-02-03-ball-state.html\n",
      "/cbb/boxscores/2017-02-03-central-michigan.html\n",
      "/cbb/boxscores/2017-02-03-columbia.html\n",
      "/cbb/boxscores/2017-02-03-cornell.html\n",
      "/cbb/boxscores/2017-02-03-dartmouth.html\n",
      "/cbb/boxscores/2017-02-03-davidson.html\n",
      "/cbb/boxscores/2017-02-03-harvard.html\n",
      "/cbb/boxscores/2017-02-03-monmouth.html\n",
      "/cbb/boxscores/2017-02-03-rider.html\n"
     ]
    }
   ],
   "source": [
    "# Make gamesheet soup\n",
    "gamesheet_url = \"http://www.sports-reference.com/cbb/boxscores/index.cgi?month=02&day=03&year=2017\"\n",
    "r = requests.get(gamesheet_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Bonus Tip: Make ROOT_URL a global variable so you don't have to keep specifying it. \n",
    "ROOT_URL = \"http://www.sports-reference.com\"\n",
    "\n",
    "# I used the SelectorGadget to learn the class \"teams\" corresponds to boxscores\n",
    "for tag in soup.find_all(class_='right gamelink'):\n",
    "    print(tag.a.get('href'))\n",
    "#     box_score_url = ROOT_URL + tag.a.get('href')\n",
    "#     print(box_score_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go! Now we can just save the htmls for each link locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tip 5\n",
    "\n",
    "We may be tempted to navigate to other gamesheets the same way. But notice the URL takes date parameters.\n",
    "\n",
    "> http://www.sports-reference.com/cbb/boxscores/index.cgi?month=01&day=22&year=2017 \n",
    "\n",
    "We can just scrape the scores for a given date and use the date we specified. This solves the additional problem of navigating to other dates. It will also makes the code _much_ easier to read and debug later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tip 6\n",
    "\n",
    "URL parameters rarely require a specific order. To make it more legible, we can flip the order of month, day, year to a less ambiguous format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Format:      http://www.sports-reference.com/cbb/boxscores/index.cgi?month=1&day=1&year=2017\n",
      "International Format: http://www.sports-reference.com/cbb/boxscores/index.cgi?year=2017&month=1&day=27\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.sports-reference.com/cbb/boxscores/index.cgi?month={}&day={}&year={}\"\n",
    "print('American Format:     ', url.format(1,1,2017))\n",
    "\n",
    "# we can even flip the parameters around to make our url less ambiguous for unamerican communists.\n",
    "url = \"http://www.sports-reference.com/cbb/boxscores/index.cgi?year={year}&month={month}&day={day}\"\n",
    "print(\"International Format:\", url.format(year=2017,month=1,day=27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Architecture\n",
    "\n",
    "It'll be helpful to use our scraper as a class. Let's make a scraper class for web scraping from different urls.\n",
    "\n",
    "Consider this docstring from the `Scraper` class in the `Scraper.py` file. \n",
    "\n",
    "```\n",
    "Scraper:\n",
    "    \n",
    "    Attributes:\n",
    "        _crawl_delay (int) -- Crawl delay to implement between requests.\n",
    "        _encoding (str) -- Which encoding to use when writing html to disk. (default 'utf-8')\n",
    "        _default_headers (dict) -- Default headers when not using VPN.\n",
    "        _VPN_headers (dict) -- Headers to be used when using VPN.\n",
    "    \n",
    "    Methods:\n",
    "        __init__ (None) -- Initialize scraper.\n",
    "        set_crawl_delay (None) -- Set crawl delay to a positive int.\n",
    "        make_soup (BeautifulSoup object) -- Make soup out of given url.\n",
    "        write_html (None) -- Write given url to disc at specified path. \n",
    "\n",
    "    Todo:\n",
    "        Give option for randomized crawl delay. Adds protection against bot detectors.\n",
    "        Consider new procedures for different encodings. Converting r.content to soup for utf-8 is wasteful.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scraper with the above structure allows the user to access all of its functionality at a glance. If we need to change anything about the scraper, we need only change one file - not two, three, four, or several. \n",
    "\n",
    "We'll do the same with our `Miner` class in the data mining workshop. There we'll tackle class inheritance. I promise that's more intimidating than difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invitation to Edit\n",
    "\n",
    "Please copy any of the code in this repo and use it in your own code base. Better yet, make changes! Play with the code and make improvements. That's how you learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
